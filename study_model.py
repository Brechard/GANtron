"""
In order to study a model the steps are:
    1. Inference samples that we force a style or emotion
    2. Compute the wav file with WaveGlow
    3. Extract the mel spectrogram with librosa
    4. Take the values to the range [0, 1] dividing by 80 and summing 1.
    5. Classify the samples with a Classifier trained on VESUS
    6. Compare the classification results with the group they were suppose to belong to.
"""
import argparse
import os

import numpy as np
import soundfile as sf
import torch

from hparams import HParams
from inference_samples import force_style_emotions
from text import text_to_sequence
from train import load_model
from utils import str2bool


def inference_samples(output_path, hparams, text):
    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]
    sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()
    speaker = None if args.hparams is None else torch.LongTensor([hparams.speaker]).cuda()

    gantron, _ = load_model(hparams)
    gantron.load_state_dict(torch.load(hparams.gantron_path)['state_dict'])
    gantron.cuda().eval()
    force_emotions = hparams.use_labels
    force_noise = hparams.use_noise
    if hparams.force_emotions is not None:
        force_emotions = hparams.force_emotions
    if hparams.force_noise is not None:
        force_noise = hparams.force_noise

    force_style_emotions(gantron, input_sequence=sequence, output_path=f"{output_path}/GANtronInference/",
                         speaker=speaker, force_emotions=force_emotions, force_style=force_noise)


def compute_wav(output_path, hparams):
    waveglow = torch.load(hparams.waveglow_path)['model']
    waveglow.cuda().eval().half()
    for k in waveglow.convinv:
        k.float()

    for p in os.listdir(f"{output_path}/GANtronInference/"):
        mel_spectrogram = np.load(f"{output_path}/GANtronInference/{p}", allow_pickle=True)
        with torch.no_grad():
            audio = waveglow.infer(mel_spectrogram.half(), sigma=0.666)
            sf.write(f"{output_path}/WaveGlowInference/{p.split('.')[0]}.wav",
                     audio[0].to(torch.float32).data.cpu().numpy(), 22050)


def study_model(output_path, hparams, text):
    inference_samples(output_path, hparams, text)
    compute_wav(output_path, hparams)
    pass


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-g', '--gantron_path', type=str, required=True, help='GANtron checkpoint path')
    parser.add_argument('-w', '--waveglow_path', type=str, required=True, help='WaveGlow checkpoint path')
    parser.add_argument('-c', '--classifier_path', type=str, required=True, help='Classifier checkpoint path')
    parser.add_argument('-o', '--output_path', type=str, required=True, help='Folder to save the comparison')
    parser.add_argument('--samples', type=int, default=20, help='Number of samples to generate')
    parser.add_argument('--hparams', type=str, required=False, help='comma separated name=value pairs')
    parser.add_argument('--speaker', default=0, type=int, required=False, help='Speaker to use when generating')
    parser.add_argument('--force_emotions', default=None, type=str2bool, help='Force using/not labels when generating')
    parser.add_argument('--force_noise', default=None, type=str2bool, help='Force using/not noise when generating')

    args = parser.parse_args()
    os.makedirs(args.output_path, exist_ok=True)
    for folder in ['GANtronInference', 'WaveGlowInference', 'ClassifierOutput']:
        os.makedirs(f'{args.output_path}/{folder}', exist_ok=True)

    hparams = HParams()
    hparams.add_params(args)

    study_model(args.output_path, hparams, text="This voice was generated by a machine")
