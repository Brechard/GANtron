import argparse
import sys

from tqdm import tqdm

# matplotlib.use('GTK3')
sys.path.append('WaveGlow/')
import numpy as np
import torch

from hparams import HParams
from train import load_model
from text import text_to_sequence
import soundfile as sf
import os


def load_GANtron(path):
    hparams = HParams(args.hparams)
    hparams.add_params(args)

    model, _ = load_model(hparams)
    model.load_state_dict(torch.load(path)['state_dict'])
    model.cuda().eval()
    return model, hparams


def load_waveglow(path):
    waveglow = torch.load(path)['model']
    waveglow.cuda().eval().half()
    for k in waveglow.convinv:
        k.float()
    return waveglow


def generate_audio(waveglow, mel_spectrogram):
    with torch.no_grad():
        audio = waveglow.infer(mel_spectrogram.half(), sigma=0.666)
    return audio


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--checkpoint_path', type=str, required=True, help='GANtron checkpoint path')
    parser.add_argument('--generate_audio', action='store_true', help='Generate the audio files')
    parser.add_argument('-w', '--waveglow_path', type=str, required=False, help='waveglow checkpoint path')
    parser.add_argument('-o', '--output_path', type=str, required=True, help='Model name to save the ')
    parser.add_argument('--samples', type=int, default=200, help='Number of samples to generate')
    parser.add_argument('--hparams', type=str, required=False, help='comma separated name=value pairs')

    args = parser.parse_args()

    os.makedirs(args.output_path, exist_ok=True)

    gantron, hparams = load_GANtron(args.checkpoint_path)
    waveglow = None
    if args.generate_audio:
        waveglow = load_waveglow(args.waveglow_path)

    text = "This voice was generated by a machine"
    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]
    sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()
    speaker = None if args.hparams is None else torch.LongTensor([1]).cuda()

    for i in tqdm(range(args.samples)):
        style = torch.rand(1, 1, hparams.noise_size)
        style = style.repeat_interleave(sequence.size(1), dim=1).cuda()
        emotions = None
        if hparams.use_labels:
            emotions = torch.rand(1, 5).cuda()

        mel_outputs, mel_outputs_postnet, _, alignments = gantron.inference(sequence, style, emotions=emotions,
                                                                            speaker=speaker)
        if waveglow is not None:
            audio = generate_audio(waveglow, mel_outputs_postnet)
            sf.write(f'{args.output_path}/{i}.wav', audio[0].to(torch.float32).data.cpu().numpy(), 22050)
        else:
            np.save(f'{args.output_path}/{1000 + i}.npy', mel_outputs_postnet[0].data.cpu().numpy())
